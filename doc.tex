\begin{abstract}

\textbf{Abstract}: Null hypothesis significance testing remains popular despite decades of concern about misuse and misinterpretation. We believe that a significant part of the problem is due simply to language: significance testing has little to do with other meanings of the word ``significance''. Although null-hypothesis tests have limitations, we argue here that they remain useful in many contexts as a guide to whether a certain effect can be seen \emph{clearly} in that context (e.g. whether we can clearly see that a correlation or between-group difference is positive or negative). We therefore suggest that p-values resulting from null-hypothesis tests be described using the language of ``statistical clarity'' rather than ``statistical significance''. We believe that this simple linguistic change has the potential to substantially enhance clarity in statistical communication.

\end{abstract}

\flushbottom
\maketitle
\newpage
\thispagestyle{empty}

\clearpage

\section*{Introduction}

For nearly a century statisticians and scientists have described the shortcomings of Null Hypothesis Significance Testing (NHST) \citep[see][]{Cohen1994}.
Books and articles proposing a deemphasis of the p~value in favor of effect sizes and confidence intervals have been cited thousands of times \citep{Cohen1994, Goodman1999, Wilkinson1999, ZiliakandMcCloskey2008, WassersteinandLazar2016}. This work includes pleas for an increased emphasis on effect sizes and confidence intervals, and points out that a zero, ``null effect'' does not exist in the vast majority of scientific fields \citep{Meehl1990, Tukey1991, Cohen1994}. Yet, p-values with out confidence intervals (or even effect sizes) and references to a ``null effect'' are still pervasive in high impact journals. 

In a 2017 meta-analysis of 356 studies \citet{Bernardietal.2017} found that 72\% of studies contained an ambiguous use of the term ``significant'', 49\% interpreted non-significant effects as zero effects, and 44\% failed to report an effect size in a comprehensible way. The misuse and misinterpretation of NHST is so frequent that there has been a recent push for drastically reducing \citep{SzucsandIoannidis2017} or abandoning \citep{McShaneetal.2017} its use. Others have called for a more strenuous significance threshold (e.g. $p < 0.005$: \citealt{Benjaminetal.2018}); however, this change is unlikely to reduce the misuse and misinterpretation of p~values, or the publication bias imposed by any p~value threshold \citep{Ridleyetal.2007}.

\citet{Kramer2011} points out that \citep{ZiliakandMcCloskey2008} ask how could NHST possibly have survived "..if it is as idiotic as we and other critics have so long believed". We believe that NHST itself is not as idiotic as suggested; the idiocy associated with NHST is the language we use in describing it. A strong argument in favor of NHST is that testing the null hypothesis (even though we often know that it's false, and can never prove that it's true) is a proxy for asking whether we can see a difference clearly. In many cases, this comes down simply to whether we can be confident of the \emph{sign} of a difference or a correlation coefficient \cite{robinson2001past}. 

\section*{Statistical Clarity}

We propose that using the language of ``statistical clarity'' would help researchers escape the trap of accepting the null hypothesis, reduce multiple types of misinterpretation associated with accepting the null hypothesis, and allow for the continued use of NHST as a method of inference for for whether we can be confident of the \emph{sign} of a difference (e.g. see \citealt{Abelson1997} for arguments for NHST). The use of ``significance'' to describe the results of hypothesis tests is deeply, and sometimes subtly, misleading, because it is at odds with other meanings of the word: the P value is not an accurate gauge of whether a result is large in magnitude, biologically important, or relevant. ``Clarity,'' on the other hand, is an apt term for what hypothesis tests evaluate. \citet{Jones2000} and \citet{Robinson2001} suggest that researchers should report $p < 0.05$ using language such as ``the direction of the differences among the treatments was undetermined''. While this is a step in the right direction, replacing ``significance'' with ``clarity'' is a more complete, direct, and universal change that will make sloppy thinking and misleading communication more difficult.

We often see sentences like, ``X et al. showed that there is no significant effect of Y on Z'' with the implication that this effect can now be assumed to be absent (or unimportant). In fact, the sentence is erroneous even before we get to the implication: X et al.'s significance test provides information about their study, not about the underlying processes \citep{HoenigandHeisey2001}. Indeed, because a p~value is a property of the data, and a true zero, $p < 0.05$ can be obtained for a very small effect with a large enough sample size. A better formulation would be ``X et al. did not find a significant effect of Y on Z.'' This mistake seems harder to make using the language of clarity:  ``X et al. showed that the effect of Y on Z is unclear'' is awkward. Similarly, ``We did not find a clear difference in response between the control and sham groups'' seems both more colloquially accurate and harder to transform into something misleading than ``We did not find a significant difference\ldots''. \citet{Bernardietal.2017} argued that ``...sociological and social significance are sacrificed on the altar of statistical significance''. Describing statistical tests in terms of clarity could allow ``significant'' to reclaim its common English definition, and reduce conflation between statistical results and substantive significance.

Descriptions of statistical results using the language of clarity should begin with reference to the effect. For example, ``The difference between the control and treatment group was not statistically clear.'' In Table~1 we provide examples from the literature of language that misinterprets p~values in three different ways, and provide a sensible way of rewriting these statements to reduce confusion.

\setlength\tabcolsep{1cm}.
\begin{tabular}{p{7.0cm}p{7.0cm}}
Language from published articles & Rewritten using ``clarity'' \\
\hline\\

\multicolumn{2}{c}{$\mathbf{p > 0.05 \neq}$ \textbf{zero effect}} \\

Toxins accumulate after acute exposure but have no effect on behaviour
& Toxins accumulate after acute exposure but effects on behaviour are statistically unclear
\\

There was no effect of elevated carbon dioxide on reproductive behaviors 
& The effect of elevated carbon dioxide on reproductive behaviors was statistically unclear
\\

The finding that species richness showed no significant relationship with the area of available habitat is surprising because richness is usually strongly influenced by landscape context 
& Although species richness is usually strongly influenced by landscape context, in the present study this relationship was not statistically clear 
\\ \\

\multicolumn{2}{c}{\textbf{Comparing P~values}}
\\

\ldots\ differences between treatment and control groups were nonsignificant, with P values of at least 0.3, and most in the range $0.7 \leq P \leq 0.9$.
& \ldots\ differences between treatment and control groups were not statistically clear ($P > 0.05$)
\\ \\

\multicolumn{2}{c}{\textbf{The difference between ``clear'' and ``not clear'' is ``not clear''}}
\\

This correlation was significant in males ($\rho=0.35$, P \textless 0.05) but not females ($\rho=0.35$, NS). \ldots [Afterwards the assumption is that a difference between males and females has been demonstrated]
& Although males and females show the same correlation coefficient ($\rho=0.35$), the sign of the coefficient is statistically clear only in males  \ldots [This phrasing may suggest to the authors that confidence intervals are called for.] 
\\

...risk of low BMD [bone mineral density] remained greater in HCV-coinfected women versus women with HIV alone
(adjusted OR 2.99, 95\% CI 1.33–6.74), but no association was found between HCV coinfection and low BMD in men 
(adjusted OR 1.26, 95\% CI 0.75–2.10). ... While the authors do not directly compare men and women they do state: 
The precise mechanisms for the association between viral hepatitis and low BMD in HIV-infected women but not men 
remain unclear.
& 
..risk of low BMD [bone mineral density] remained greater in HCV-coinfected women versus women with HIV alone
(adjusted OR 2.99, 95\% CI 1.33–6.74), but the association between HCV coinfection and low BMD in men was not
statistically clear (adjusted OR 1.26, 95\% CI 0.75–2.10). ... Pursuing the biological importance of HIV on BMD in 
women and not men is premature given these statistics and requires further study.
\\
\end{tabular}
   \captionof{table}{Rewriting misinformed language in the published literature using our proposed updated language.}

\section*{Closing Comments}

\subsection*{Non-inferiority tests}

Following a shift in language, we suggest using a non-inferiority test to test for a small effect. Consider the following example. A hospital is considering switching from bulky, expensive surgical face masks to lighter weight, cheaper masks. A clinical trail that tests the effectiveness of both masks and finds $p > 0.05$ is not a success; this tells the hospital that the difference between the masks is not statistically clear! What the hospital actually want to know is whether thecheaper, lighter masks are \emph{only a little bit worse than the bulky, expensive masks}. This requires a $H_{0}$ that defines the target difference in effectiveness of the two masks that the hospital is willing to accept to make the transition. We note that this statistical test should be set up as a one-tailed test; the logical, \emph{a priori} assumption is that the bulky, expensive masks are more effective. In this case, a $p < 0.05$ tells the hospital that the difference between the new masks and old masks is small and that it they can proceed with the switch.

\subsection*{Focal vs non-focal hypotheses}

Accepting the null is especially common in diagnostic tests. For example, a Shapiro-Wilk test of normality that returns $p > 0.05$ means that it is statistically unclear how the data departs from normality (the data is collected from a complicated world and does depart from normality). Without the goal of picking on any paper in specific, ``...allowing us to accept the null hypothesis about the normal spot count distribution.'' \citep{Karulinetal.2015} is common language that should be avoided.

\subsection*{Previous suggestions are compatible with a change in language}

Following the lead of \citet{Cohen1994} and others \citep{Goodman1999, ZiliakandMcCloskey2008, WassersteinandLazar2016}, we conclude with a plea for an increased emphasis on effect sizes and confidence intervals. However, we suggest that the use of ``statistical clarity'' forces the \emph{need} for confidence intervals and effect sizes by removing ambiguity from statements associated with $p < 0.05$. The statement ``The difference between our control and treatment groups was statistically clear ($p = 0.007$)'' is unambiguously incomplete; an effect size and confidence interval is required to complete the story.

We echo previous statements in favor of ``neglected factors'' (prior and related evidence, plausibility of mechanism, study design and data quality, real world benefits, novelty and other factors) \citep{McShaneetal.2017} and reporting of \emph{a priori} analysis of statistical power to avoid emphasis on implausibly large effects given low statistical power \citep[the "winners curse"][]{GelmanandCarlin2014, SzucsandIoannidis2017, Bernardietal.2017}.  Additionally, we are in favor of writing a statistical journal that chronicles all steps in the analysis process \citep{Kassetal.2016}, and clearly defining the boundary between results presented as inferences based on \emph{a priori} hypotheses and those from \emph{post hoc} data exploration.

\subsection*{Acknowledgements}

We thank members of the Dushoff and Bolker labs for helpful comments on the first draft of the manuscript.


