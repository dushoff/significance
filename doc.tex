\begin{abstract}

\mk{Unedited placeholder text}

\textbf{Abstract}: Decades of misuse and misinterpretation of Null Hypothesis Significance Testing (NHST) has led to recent 
calls for abandoning its use. We argue that statistical hypothesis tests are useful, but are not, and should not be 
referred to, as tests of significance. We propose that a sensible method to reduce misconceptions about p~values 
is to refer to the results of NHST (i.e. $p < or > 0.05$) using ``statistical clarity''. A test of the null hypothesis tells 
us whether or not we can \emph{clearly} see our effect (e.g. whether a correlation is positive or negative, or if a response 
in the treatment group is greater than the response in the control group). Specifically, we argue the use of ``statistical
clarity'' will help researchers avoid accepting the null hypothesis and retrieve the English definition for ``significance'' 
to shift focus back to substantive significance.

\textbf{Structure:} I like the idea of two parallel boxes that have examples of language used in published papers on the
left and corrected language on the right. Maybe three examples that correspond to four primary misinterpretations people
make (zero effect, large p-values as ``more'' evidence for something, difference between clear and not clear is not clear,
substantive significance)

\textbf{Previous Work:} The bottom line is that \emph{\textbf{a lot}} has been said on the general topic of NHST and 
p-values, though the idea of changing the language to clarity and retrieving the English definition of significant
is novel. \citet{Bernardietal.2017} and \citet{McShaneetal.2017} have covered a lot of the history that is
recounted below. It may be best to just cite these papers and state that concern about NHST and p-value abuse 
remains high. \citep{Bernardietal.2017} present an analysis of the presence of errors in the interpretation of 
results and ambiguity in language focused on NHST. Out of  They point out that these results are similar to the
results from a similar review in the field of economics \citep{ZiliakandMcCloskey2008}.

\textbf{Notes:} A lot of this language assumes $H_{0} = 0$. Probably need to address clearly that many of these cited
papers are based in psychology. I imagine we will want to anonymize quotes? (Though I kind of want to call out the null
result short courses paper --- maybe as a point about non-significance and calling for reform)

\textbf{Venue:} \mk{My list, no input from PIs on this yet}. No idea... Forum in Trends in E and E: max 1200 words?
Note in The American Naturalist: max 3000 words? Perspectives article in Nature?

\end{abstract}

\flushbottom
\maketitle
\newpage
\thispagestyle{empty}

\section*{Introduction}

For nearly a century statisticians and scientists have described the shortcomings of Null Hypothesis Significance Testing
(NHST) \citep[see][]{Cohen1994}. Research articles and books written in the past two decades proposing a deemphasis of the 
p~value in favor of effect sizes and confidence intervals have been cited thousands of times \citep{Cohen1994, Goodman1999, 
Wilkinson1999, ZiliakandMcCloskey2008, WassersteinandLazar2016}. Yet, both a reliance on NHST and the misinterpretation of 
results obtained from NHST remain high. In a 2017 meta-analysis of 356 studies published in \emph{European Sociological 
Research}, \citet{Bernardietal.2017} found that 72\% of studies contained an ambiguous use of the term ``significant'', 
49\% interpreted non-significant effects as zero effects, and 44 \% failed to report an effect size in a comprehensible 
way. The misuse and misinterpretation of NHST is so pervasive that there has been a recent push for drastically reducing its 
use \citep{SzucsandIoannidis2017} or abandoning its use altogether \citep{McShaneetal.2017}. Others have called for a more
strenuous significance threshold (e.g. $p < 0.005$: \citealt{Benjaminetal.2018}); however, this change is unlikely to reduce
the misuse and misinterpretation of p~values, or the publication bias imposed by any p~value threshold 
\citep{Ridleyetal.2007}

Alternatively, we suggest a change in the language used in NHST. Results from NHST (i.e. $p < or > 0.05$) should be 
described using language that directly corresponds to what a test of the null hypothesis actually tells us: whether or not 
we can \emph{clearly} see our effect. For example, given $H_{0} = 0$ (e.g. the difference in our treatments is 0) what a 
$p < 0.05$ tells us is whether or not we can clearly see the sign of the effect. Using the language of ``statistical 
clarity'' would help researchers escape the trap of accepting the null hypothesis and reduce multiple types of
misinterpretation associated with accepting the null hypothesis, while allowing for it to be retained as a method of 
inference (e.g. see \citealt{Abelson1997} for arguments for NHST). For example,equating $p < 0.05$ with a zero effect would 
be much harder to make once a researcher has written the following sentence: ``The difference between our control and 
treatment group was not statistically clear ($p = 0.24$). Additionally, the use of $p > 0.05$ as evidence for something 
(accepting the null hypothesis) would be more difficult if the following language was used: ``We were unable to clearly 
determine how our control and sham treatments differed''. Additionally, once ``significant'' regained its common English 
definition, conflating $p < 0.05$ with substantive significance would be more difficult.

\begin{itemize}
  \item Other topics to mention
  	\subitem difficulty in interpreting confidence intervals?
    \subitem usefulness of p --- use in splines: non-ridiculous standard to determine if we can see a pattern
    \subitem a p~value is not a property of reality (a biological ``thing''), but a property of a data set and assumptions...
\end{itemize}

\begin{itemize}
  \item To do
  	\subitem unsort bib for convenient lookup of types of cites
    \subitem links in bib to pdfs
\end{itemize}

\section*{Statistical Clarity}

\mk{Recount the way a high p is obtained?}

\subsection*{$\mathbf{p > 0.05 \neq}$ zero effect}

In the vast majority of scientific fields a truly zero, ``null effect'' does not exist \citep{Meehl1990, Tukey1991,
Cohen1994}. Yet, references to a null effect can still be found in high impact journals \citep[e.g.][]{Feldonetal.2017}. 
To emphasize that effects are not truly zero, descriptions of statistical results using the language of clarity should
begin with reference to the effect. For example, ``The difference between the control and treatment group was not 
statistically clear.'' 

\begin{itemize}
  \item Equating $p > 0.05$ with no difference: \citep{Ortegoetal.2007, Singhetal.2017}
\end{itemize}

\mk{Probably do not need this full description here}

Following a shift in language, we suggest using a non-inferiority test to test for a small effect. Consider the following 
example. A hospital is considering switching from bulky, expensive surgical face masks to lighter weight, cheaper masks. A
clinical trail that tests the effectiveness of both masks and finds $p > 0.05$ is not a success; this tells the hospital 
that the difference between the masks is not statistically clear! What the hospital actually want to know is whether the 
cheaper, lighter masks are \emph{only a little bit worse than the bulky, expensive masks}. This requires a $H_{0}$ that 
defines the target difference in effectiveness of the two masks that the hospital is willing to accept to make the 
transition. We note that this statistical test should be set up as a one-tailed test; the logical, \emph{a priori} 
assumption is that the bulky, expensive masks are more effective. In this case, a $p < 0.05$ tells the hospital that the 
difference between the new masks and old masks is small and that it they can proceed with the switch.

\begin{itemize}
  \item ``...establish the \emph{a priori} selection of an effect level that is considered unacceptably toxic:''
  \citep{Dentonetal.2011}
\end{itemize}

\subsection*{Using larger p~values as more evidence for something}

Another common mistake is using a larger p~value as stronger evidence for accepting the null hypothesis.  For example, 
\mk{Need a good quote here. An ok placeholder:} ``...yielded nonsignificant differences on measures of those skills with P 
values of at least 0.2, and most in the $0.7 \leq P \leq 0.9$ range.'' \citep{Feldonetal.2017}. A larger p value actually
corresponds to less clarity and thus the possibility of an undetected larger effect. 

\mk{Point about focal vs non-focal hypotheses. Often accept the null for non-focal (diagnostics). While this is not
appropriate language it seems less bad...}

\begin{itemize}
  \item Setting out to accept the null: \citep{Karulinetal.2015}
\end{itemize}  

\mk{Again, maybe this is too much}

Accepting the null is especially common in diagnostic tests. For example, a Shapiro-Wilk test of normality that
returns $p > 0.05$ means that it is statistically unclear how the data departs from normality (the data is collected
from a complicated world and does depart from normality). Without the goal of picking on any paper in specific, 
``...allowing us to accept the null hypothesis about the normal spot count distribution.'' \citep{Karulinetal.2015} 
is common language that should be avoided.

\subsection*{The difference between clear and not clear is not clear}

In 2006 Gelman and Stern wrote that the difference between statistically significant and statistically non-significant is
not itself significant. Rewritten using ``clarity'' this statement indicates that if a single result is not statistically 
clear, any comparison made between that unclear result and a clear result will not itself be a clear comparison. An example
taken from the literature is the following: 

For example, if a treatment induces a statistically clear increase in growth rate in males, but has an unclear effect in 
females, the relationship between the treatment on males and females is not clear. 

\begin{itemize}
  \item With the \emph{correlation coefficient} for males and females (but significance for only one sex---``substrate 
  choice correlated significantly with food choice in males'') the authors go on to describe biologically why the 
  correlation would be significant for males but not females. \citep{MerilaitaandJormalainen1997}
\end{itemize}  

\subsection*{Conflating statistical and substantive significance}

In almost all cases what a researcher really cares about is the size of an effect and the scientific implications of the
effect. Statistical significance does not, in and of itself, tell us anything about substantive significance. As 
\citet{Bernardietal.2017} state, ``...sociological and social significance are sacrificed on the altar of statistical 
significance''. Included in this is an absence of reported effect sizes and confidence intervals.

\begin{itemize}
  \item Lack of clarity differentiating between biological and statistical significance: {Simonetal.2017}
  \item Complete lack of effect sizes: \citep{Juriadoetal.2017}
\end{itemize}

\section*{Closing Comments}

\begin{itemize}
  \item Topics we may want to bring up
	  \subitem Including a p~value helps to avoid type-S errors
\end{itemize}

Following the lead of \citet{Cohen1994} and others \citep{Goodman1999, ZiliakandMcCloskey2008, WassersteinandLazar2016},
we conclude with a plea for an increased emphasis on effect sizes and confidence intervals. However, we suggest that the
use of ``statistical clarity'' forces the \emph{need} for confidence intervals and effect sizes by removing all ambiguity
from statements associated with $p < 0.05$. The statement ``The difference between our control and treatment groups were
statistically clear ($p = 0.007$)'' in unambiguously incomplete and uncompelling; an effect size and confidence interval 
is required to complete the story.

\mk{Not sure if we want any of this and how much we want to expand on any of these topics:}

We echo previous statements in favor of ``neglected factors'' (prior and related evidence, plausibility of mechanism, 
study design and data quality, real world benefits, novelty and other factors) \citep{McShaneetal.2017} and 
reporting of \emph{a priori} analysis of statistical power to avoid emphasis on implausibly large effects given low 
statistical power \citep[the "winners curse"][]{GelmanandCarlin2014, SzucsandIoannidis2017, Bernardietal.2017}. 
Additionally, we are in favor of writing a statistical journal that chronicles all steps in the analysis process 
\citep{Kassetal.2016}, and clearly defining the boundary between results presented as inferences based on \emph{a priori}
hypotheses and those from \emph{post hoc} data exploration. 

\subsection*{Acknowledgements}

We (probably will) thank the Bolker lab for helpful comments on the first draft of the manuscript. 

